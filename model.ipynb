{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5be86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler, Dataset\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf485ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to dataset and csv\n",
    "CSV_PATH = 'top50.csv'\n",
    "IMAGE_FOLDER_PATH = 'top50images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594891af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom pytorch dataset for our top 50 cities dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path, image_folder, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path: path to the csv file with labels.\n",
    "            image_folder: directory of images\n",
    "            transform: transform to be applied on images.\n",
    "        \"\"\"\n",
    "        self.img_labels = pd.read_csv(csv_path)\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "        # encode cities to labels\n",
    "        self.unique_cities = self.img_labels['city'].unique()\n",
    "        self.city_to_idx = {city: i for i, city in enumerate(self.unique_cities)}\n",
    "        self.idx_to_city = {i: city for i, city in enumerate(self.unique_cities)}\n",
    "        \n",
    "        print(f\"Mapped {len(self.unique_cities)} unique classes (cities)\") # should be 50\n",
    "        # print(self.city_to_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns length of the dataset (number of images)\n",
    "        \"\"\"\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a data sample for a given index.\n",
    "\n",
    "        Args:\n",
    "            idx: index of item/image\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, label) where image is the transformed image tensor\n",
    "                   and label is the encoded label of the city.\n",
    "        \"\"\"\n",
    "        # get image file name and its city from dataframe\n",
    "        row = self.img_labels.iloc[idx]\n",
    "        image_filename = row['filename']\n",
    "        city_label_str = row['city']\n",
    "        \n",
    "        # construct path to image\n",
    "        image_path = os.path.join(self.image_folder, image_filename)\n",
    "        \n",
    "        # load image\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error, could not find image at path: {image_path}\")\n",
    "            return None, -1 \n",
    "\n",
    "        # get encoded label of city\n",
    "        label = self.city_to_idx[city_label_str]\n",
    "\n",
    "        # apply necessary transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f47aa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset...\n",
      "Mapped 50 unique classes (cities)\n"
     ]
    }
   ],
   "source": [
    "# transform data for resnet\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),      # resize to 224 x 224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(               # normalize to imagenet stats\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# create dataset\n",
    "print(\"Creating dataset...\")\n",
    "full_dataset = CityImageDataset(\n",
    "    csv_path=CSV_PATH,\n",
    "    image_folder=IMAGE_FOLDER_PATH,\n",
    "    transform=data_transforms\n",
    ")\n",
    "\n",
    "# split data into 80/20 for train/val\n",
    "dataset_size = len(full_dataset)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "val_size = dataset_size - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "660b1c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 8023\n",
      "Training samples: 6418\n",
      "Validation samples: 1605\n"
     ]
    }
   ],
   "source": [
    "# seed and split\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "print(f\"Total samples: {dataset_size}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f11ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts in training set: Albuquerque         132\n",
      "Atlanta             137\n",
      "Austin              142\n",
      "Bakersfield         147\n",
      "Baltimorem           79\n",
      "Boston               46\n",
      "Charlotte           146\n",
      "Chicago             128\n",
      "Colorado Springs    127\n",
      "Columbus            154\n",
      "Dallas              158\n",
      "Denveri             144\n",
      "Detroit             136\n",
      "El Paso              75\n",
      "Fort Worth          111\n",
      "Fresno              123\n",
      "Houston             142\n",
      "Indianapolisg       140\n",
      "Jacksonvillef       145\n",
      "Kansas City         141\n",
      "Las Vegas           125\n",
      "Long Beach          124\n",
      "Los Angeles         147\n",
      "Louisvillel         120\n",
      "Memphis             147\n",
      "Mesa                 91\n",
      "Miami               151\n",
      "Milwaukee           152\n",
      "Minneapolis         148\n",
      "Nashvillej          134\n",
      "New Yorkd           152\n",
      "Oakland             119\n",
      "Oklahoma City       142\n",
      "Omaha               138\n",
      "Philadelphiae       161\n",
      "Phoenix             141\n",
      "Portland            154\n",
      "Raleigh             146\n",
      "Sacramento          142\n",
      "San Antonio         163\n",
      "San Diego            97\n",
      "San Franciscoh      155\n",
      "San Jose            146\n",
      "Seattle             135\n",
      "Tampa               152\n",
      "Tucson               42\n",
      "Tulsa               135\n",
      "Virginia Beachm       5\n",
      "Washingtonk          70\n",
      "Wichita             131\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# weight samples to try to have balanced training batches\n",
    "# get labels for training set samples\n",
    "train_labels = [full_dataset.img_labels.iloc[i]['city'] for i in train_dataset.indices]\n",
    "\n",
    "# count occurrences of each class\n",
    "class_counts = pd.Series(train_labels).value_counts().sort_index()\n",
    "print(f\"Class counts in training set: {class_counts}\")\n",
    "\n",
    "# calc weight for each class\n",
    "class_weights = 1.0 / torch.tensor(class_counts.values, dtype=torch.float)\n",
    "\n",
    "# create weight for each training sample\n",
    "sample_weights = torch.tensor([class_weights[full_dataset.city_to_idx[label]] for label in train_labels])\n",
    "\n",
    "# create weighted sampler\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a32399f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "Number of training batches: 201\n",
      "Number of validation batches: 51\n"
     ]
    }
   ],
   "source": [
    "# create data loaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# training set loader\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=train_sampler,  # let weighted sampler do sampling\n",
    "    num_workers=0,          # change to higher number on colab since mac is dumb\n",
    "    shuffle=False           # need to set shuffle to false when using a sampler\n",
    ")\n",
    "\n",
    "# validation set loader\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0           # change to higher number on colab since mac is dumb\n",
    ")\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df07cd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# get device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, loss_func, optimizer, train_loader, val_loader, device, num_epochs=5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: model to train\n",
    "        loss_func: loss function\n",
    "        optimizer: optimizer\n",
    "        train_loader: DataLoader for the training set\n",
    "        val_loader: DataLoader for the validation set\n",
    "        device: the torch.device to run on\n",
    "        num_epochs: number of epochs\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"--- Starting Training ---\")\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # keep track of how much time each epoch takes\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # TRAINING\n",
    "        model.train()  # set to training mode\n",
    "        running_loss = 0.0  # keep track of loss\n",
    "\n",
    "        # iterate through batches\n",
    "        for images, labels in train_loader:\n",
    "\n",
    "            # move to device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # zero out gradients from previous batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # apply loss function\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # backprop\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # keep track of loss\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # keep track of total epoch loss\n",
    "        epoch_train_loss = running_loss / len(train_loader.sampler)\n",
    "\n",
    "        # VALIDATION\n",
    "        model.eval()  # set to eval mode\n",
    "        val_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        # no gradients\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # apply forward pass and evaluate output\n",
    "                outputs = model(images)\n",
    "                loss = loss_func(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                \n",
    "                # get prediction\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_predictions += torch.sum(preds == labels.data)\n",
    "        \n",
    "        # keep track of total epoch val loss\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = correct_predictions.float() / len(val_loader.dataset)\n",
    "        \n",
    "        # time it took for epoch to train\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | Time: {epoch_time:.2f}s\")\n",
    "        print(f\"\\t Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "    print()\n",
    "    print(\"--- Training Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b4379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing parameters of pre-trained layers...\n",
      "Replaced final layer, new # of output features: 50\n"
     ]
    }
   ],
   "source": [
    "# load resnet model\n",
    "resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "# model = models.resnet50(weights='DEFAULT')\n",
    "\n",
    "# freeze params/layers of model\n",
    "print(\"Freezing parameters of pre-trained layers...\")\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# replace final layer with our classification layer\n",
    "num_features = resnet50.fc.in_features\n",
    "num_classes = len(full_dataset.unique_cities)\n",
    "resnet50.fc = nn.Linear(num_features, num_classes)\n",
    "print(f\"Replaced final layer, new # of output features: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3949165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model to device\n",
    "resnet50 = resnet50.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f612ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f52ed883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up loss func and optimizer\n",
    "\n",
    "# loss func\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# will only update weights for final layer since we froze previous ones\n",
    "optimizer = optim.Adam(resnet50.fc.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "406888b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1 / 5 | Time: 127.35s\n",
      "\t Train Loss: 3.3283 | Val Loss: 3.3198 | Val Acc: 0.1614\n",
      "Epoch 2 / 5 | Time: 127.18s\n",
      "\t Train Loss: 2.7179 | Val Loss: 3.1107 | Val Acc: 0.2037\n",
      "Epoch 3 / 5 | Time: 126.48s\n",
      "\t Train Loss: 2.4101 | Val Loss: 3.0205 | Val Acc: 0.2336\n",
      "Epoch 4 / 5 | Time: 124.62s\n",
      "\t Train Loss: 2.1686 | Val Loss: 2.9591 | Val Acc: 0.2336\n",
      "Epoch 5 / 5 | Time: 120.23s\n",
      "\t Train Loss: 1.9716 | Val Loss: 2.9407 | Val Acc: 0.2417\n",
      "\n",
      "--- Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "# training/validation loop\n",
    "train_and_validate(resnet50, loss_func, optimizer, train_loader, val_loader, device, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d379f5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing parameters of pre-trained layers...\n",
    "# Replaced final layer. Output features: 50\n",
    "# Epoch 1/5 | Time: 125.85s\n",
    "# \tTrain Loss: 3.3243 | Val Loss: 3.3010 | Val Acc: 0.1657\n",
    "# Epoch 2/5 | Time: 120.36s\n",
    "# \tTrain Loss: 2.7309 | Val Loss: 3.0988 | Val Acc: 0.2206\n",
    "# Epoch 3/5 | Time: 120.52s\n",
    "# \tTrain Loss: 2.4108 | Val Loss: 3.0306 | Val Acc: 0.2206\n",
    "# Epoch 4/5 | Time: 120.46s\n",
    "# \tTrain Loss: 2.1759 | Val Loss: 2.9664 | Val Acc: 0.2374\n",
    "# Epoch 5/5 | Time: 121.21s\n",
    "# \tTrain Loss: 1.9838 | Val Loss: 2.9176 | Val Acc: 0.2480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258547ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
